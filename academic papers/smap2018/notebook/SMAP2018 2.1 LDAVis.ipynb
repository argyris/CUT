{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling Analysis \n",
    "\n",
    "This notebook was used as part of the research of using topic modelling as a means to filter out irrelevant hashtags that accompany Instagram images. \n",
    "\n",
    "Author: Argyris Argyrou, Cyprus University of Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CRISP-DM methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T13:38:49.532429Z",
     "start_time": "2018-09-04T13:38:49.520227Z"
    }
   },
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:27.898831Z",
     "start_time": "2018-09-06T09:30:27.891562Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import re\n",
    "import collections\n",
    "import wordninja "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries: For NLP, using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:28.738990Z",
     "start_time": "2018-09-06T09:30:27.902806Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries: stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:28.757399Z",
     "start_time": "2018-09-06T09:30:28.744018Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:28.777367Z",
     "start_time": "2018-09-06T09:30:28.767511Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint \n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasetâ€”corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:28.790836Z",
     "start_time": "2018-09-06T09:30:28.780987Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "glob.glob(\"../dataset/*.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T13:41:51.023546Z",
     "start_time": "2018-09-04T13:41:51.004997Z"
    }
   },
   "source": [
    "### Pre-process and vectorize the documents (hashtags)\n",
    "Among other things, we will:\n",
    "\n",
    "- Split the documents into tokens.\n",
    "- Lemmatize the tokens (e.g: horse, horses to horse)\n",
    "- Probabilistically split concatenated\n",
    "- Remove stopwords\n",
    "- Remove numbers, but not words that contain numbers\n",
    "- Remove words that are only three character\n",
    "- Compute a bag-of-words representation of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Select Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.053119Z",
     "start_time": "2018-09-06T09:30:28.800543Z"
    }
   },
   "outputs": [],
   "source": [
    "n_print = input(\"Enter the file name you like to anylize. e.g HORSE, CAR etc: \")\n",
    "\n",
    "n_print=n_print.upper();\n",
    "try:\n",
    "   dataset = '../dataset/'+n_print+'.xlsx'\n",
    "   df_dataset = pd.read_excel(dataset)\n",
    "except IOError:\n",
    "   print('hm, we don\\'t have this dataset. Please try another one')\n",
    "\n",
    "data = []\n",
    "for index, row in df_dataset.iterrows():\n",
    "    data.append(row[\"Hashtag\"])\n",
    "    \n",
    "df_dataset = pd.DataFrame(data, columns = ['Hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.062815Z",
     "start_time": "2018-09-06T09:30:34.057881Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T13:47:48.273942Z",
     "start_time": "2018-09-04T13:47:48.249997Z"
    }
   },
   "source": [
    "**Text Wrangling & Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T13:48:05.045151Z",
     "start_time": "2018-09-04T13:48:05.036897Z"
    }
   },
   "source": [
    "cleaning: stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.079561Z",
     "start_time": "2018-09-06T09:30:34.067070Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning: inst-stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.095135Z",
     "start_time": "2018-09-06T09:30:34.083537Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exec(open('../stops.py').read())  \n",
    "df_instastop = pd.DataFrame(STOPS_LIST, columns = ['Insta stopwords (preview)'])\n",
    "# df_instastop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning: lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.114179Z",
     "start_time": "2018-09-06T09:30:34.101216Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning: split multiplewords (bi-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.137396Z",
     "start_time": "2018-09-06T09:30:34.118295Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitmultiplewords (mylist):\n",
    "    \"\"\"The method's for spliting words\"\"\"\n",
    "    myninjalist = []\n",
    "    for x in mylist:\n",
    "        xtoken = x\n",
    "        if (len(wordninja.split(xtoken))>1):\n",
    "            ytoken = wordninja.split(xtoken)\n",
    "            for y in ytoken:\n",
    "                myninjalist.append(y)\n",
    "        else:\n",
    "            myninjalist.append(x) \n",
    "    return myninjalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning: Building a Text Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.159188Z",
     "start_time": "2018-09-06T09:30:34.143921Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = splitmultiplewords(tokens)        \n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    tokens = [index for index in tokens if not index in STOPS_LIST]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corpus**: prepare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.169745Z",
     "start_time": "2018-09-06T09:30:34.164785Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.354115Z",
     "start_time": "2018-09-06T09:30:34.175119Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = []  \n",
    "\n",
    "for index, row in df_dataset.iterrows():\n",
    "    line = row[\"Hashtags\"].lower()\n",
    "    tokens = prepare_text_for_lda(line)      \n",
    "    docs.append(list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.369541Z",
     "start_time": "2018-09-06T09:30:34.358396Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(docs)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary: We are creating a dictionary from the data, then convert to bag-of-words corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.386450Z",
     "start_time": "2018-09-06T09:30:34.378873Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.758355Z",
     "start_time": "2018-09-06T09:30:34.394751Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(dictionary.items(), key=lambda x: x[1]) ## Let's sort our dictionary\n",
    "pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T13:52:47.820533Z",
     "start_time": "2018-09-04T13:52:47.811209Z"
    }
   },
   "source": [
    "## Topic Modeling, LDA\n",
    "### Vectorize data\n",
    "Finally, we transform the documents to a vectorized form. We simply compute the frequency of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T13:53:03.998431Z",
     "start_time": "2018-09-04T13:53:03.989050Z"
    }
   },
   "source": [
    "Bag-of-words representation of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.767613Z",
     "start_time": "2018-09-06T09:30:34.761780Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function doc2bow() simply counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a bag-of-words--a sparse vector, in the form of [(word_id, word_count), ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:34.780962Z",
     "start_time": "2018-09-06T09:30:34.771425Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T10:17:05.301366Z",
     "start_time": "2018-05-05T10:17:05.292225Z"
    }
   },
   "source": [
    "### Discover topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:36.970530Z",
     "start_time": "2018-09-06T09:30:34.784241Z"
    }
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=20)\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:30:37.001732Z",
     "start_time": "2018-09-06T09:30:36.973773Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "num_words = 10\n",
    "\n",
    "List = ldamodel.print_topics(num_topics, num_words)\n",
    "Topic_words =[]\n",
    "for i in range(0,len(List)):\n",
    "    word_list = re.sub(r'(.\\....\\*)|(\\+ .\\....\\*)', '',List[i][1])\n",
    "    temp = [word for word in word_list.split()]\n",
    "    Topic_words.append(temp)\n",
    "    print('Topic ' + str(i) + ': ' + '' + str(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T19:51:24.176719Z",
     "start_time": "2018-09-05T19:51:24.165355Z"
    }
   },
   "source": [
    "Evaluation: **Coherence Score** a measure of how good the model is. higher the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:31:25.124170Z",
     "start_time": "2018-09-06T09:31:24.341703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(docs)\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=docs, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: **Perplexity**  a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:31:21.961188Z",
     "start_time": "2018-09-06T09:31:21.879500Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T13:55:08.334326Z",
     "start_time": "2018-09-04T13:55:08.320927Z"
    }
   },
   "source": [
    "#### LDAvis\n",
    "\n",
    "LDAVis is designed to help us interpret topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T09:31:34.218541Z",
     "start_time": "2018-09-06T09:31:31.641345Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saliency: a measure of how much the term tells you about the topic.\n",
    "- Relevance: a weighted average of the probability of the word given the topic and the word given the topic normalized by the probability of the topic.\n",
    "\n",
    "The size of the bubble measures the importance of the topics, relative to the data.\n",
    "\n",
    "First, we got the most salient terms, means terms mostly tell us about whatâ€™s going on relative to the topics. In our case **horse**, **life**,**rider**. When we have 5 or 10 topics, we can see certain topics are clustered together, this indicates the similarity between topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
